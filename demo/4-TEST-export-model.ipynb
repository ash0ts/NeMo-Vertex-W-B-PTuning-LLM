{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d755b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import wandb\n",
    "import wget\n",
    "from megatron.core import parallel_state\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_prompt_learning_model import \\\n",
    "    MegatronGPTPromptLearningModel\n",
    "from nemo.collections.nlp.modules.common import VirtualPromptStyle\n",
    "from nemo.collections.nlp.modules.common.transformer.text_generation import (\n",
    "    LengthParam, SamplingParam)\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPStrategy\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.omegaconf import open_dict\n",
    "from pytorch_lightning.plugins.environments import TorchElasticEnvironment\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from utils import download_file\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"NeMo Megatron PTuning - Evaluation\")\n",
    "    \n",
    "    parser.add_argument(\"--output_dir\", default=\"output\", help=\"Output directory.\")\n",
    "    \n",
    "    # Add argparse parameters for the `inference` block\n",
    "    # Add argparse parameters for the `inference` block with default values\n",
    "    parser.add_argument('--greedy', type=bool, default=False, help='Whether or not to use sampling; use greedy decoding otherwise')\n",
    "    parser.add_argument('--top_k', type=int, default=0, help='The number of highest probability vocabulary tokens to keep for top-k-filtering.')\n",
    "    parser.add_argument('--top_p', type=float, default=0.9, help='If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.')\n",
    "    parser.add_argument('--temperature', type=float, default=1.0, help='Sampling temperature')\n",
    "    parser.add_argument('--add_BOS', type=bool, default=True, help='Add the bos token at the beginning of the prompt')\n",
    "    parser.add_argument('--tokens_to_generate', type=int, default=30, help='The minimum length of the sequence to be generated.')\n",
    "    parser.add_argument('--all_probs', type=bool, default=False, help='Whether return the log prob for all the tokens in vocab')\n",
    "    parser.add_argument('--repetition_penalty', type=float, default=1.2, help='The parameter for repetition penalty. 1.0 means no penalty.')\n",
    "    parser.add_argument('--min_tokens_to_generate', type=int, default=0, help='The minimum length of the sequence to be generated.')\n",
    "    parser.add_argument('--compute_logprob', type=bool, default=False, help='A flag used to compute logprob of all the input text, a very special case of running inference, default False')\n",
    "    parser.add_argument('--batch_size', type=int, default=5, help='Batch size for inference')\n",
    "\n",
    "    \n",
    "    # Trainer configs\n",
    "    parser.add_argument(\n",
    "        \"--accelerator\",\n",
    "        default=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        help=\"Accelerator - GPU or CPU.\",\n",
    "    )\n",
    "    parser.add_argument(\"--devices\", default=1, type=int, help=\"Number of devices.\")\n",
    "    parser.add_argument(\"--enable_checkpointing\", action=\"store_true\",\n",
    "        help=\"Whether to enable checkpoints during inference. Default is to not to\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        default=16 if torch.cuda.is_available() else 32,\n",
    "        type=int,\n",
    "        help=\"Training precision.\",\n",
    "    )\n",
    "    \n",
    "    # Experiment manager configs\n",
    "    parser.add_argument(\n",
    "        \"--name\", default=\"NeMo_Megatron_PTuning\", help=\"Name of the experiment.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_if_exists\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to resume if exists. Default is to not resume\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no_create_wandb_logger\",\n",
    "        action=\"store_false\",\n",
    "        dest=\"create_wandb_logger\",\n",
    "        help=\"Specify this flag to not create wandb logger. Default is to create wandb logger.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--project\", default=\"NeMo_Megatron_PTuning\", help=\"WandB project name.\"\n",
    "    )\n",
    "    parser.add_argument(\"--log_model\", default=\"all\", help=\"Log model in WandB.\")\n",
    "    \n",
    "    return parser.parse_known_args()\n",
    "    \n",
    "args, unknown = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2756867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33ma-sh0ts\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/nemo/demo/wandb/run-20230816_093645-7pky2ryz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a-sh0ts/NeMo_Megatron_PTuning/runs/7pky2ryz' target=\"_blank\">export@2023-08-16_09-36-43</a></strong> to <a href='https://wandb.ai/a-sh0ts/NeMo_Megatron_PTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a-sh0ts/NeMo_Megatron_PTuning' target=\"_blank\">https://wandb.ai/a-sh0ts/NeMo_Megatron_PTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a-sh0ts/NeMo_Megatron_PTuning/runs/7pky2ryz' target=\"_blank\">https://wandb.ai/a-sh0ts/NeMo_Megatron_PTuning/runs/7pky2ryz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact squad:latest, 1497.99MB. 15 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   15 of 15 files downloaded.  \n",
      "Done. 0:0:2.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact final_model_checkpoints:latest, 1372.88MB. 5 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n",
      "Done. 0:0:2.7\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "megatron_gpt_prompt_learning_inference.yaml already exists. Skipping download.\n",
      "[NeMo I 2023-08-16 09:36:52 exp_manager:374] Experiments will be logged at /workspace/nemo/demo/nemo_experiments/default/2023-08-16_09-36-52\n",
      "[NeMo I 2023-08-16 09:36:52 exp_manager:797] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-16 09:36:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:36:53 exp_manager:812] WandBLogger has been set up\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:53 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:54 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-16 09:36:55 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:36:55 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpceshfx_g/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpceshfx_g/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-08-16 09:36:55 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpceshfx_g/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpceshfx_g/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:36:55 megatron_base_model:264] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2023-08-16 09:36:56 nlp_overrides:401] Model MegatronGPTModel was successfully restored from /workspace/nemo/demo/artifacts/final_model_checkpoints:latest/nemo_assets/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-08-16 09:36:56 auto_tokenizer:172] 15 special tokens added, resize your model accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:36:58 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-08-16 09:36:58 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-16 09:36:58 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:36:58 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmppig2bkvb/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmppig2bkvb/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-08-16 09:36:58 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmppig2bkvb/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmppig2bkvb/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:36:58 megatron_base_model:264] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo I 2023-08-16 09:36:59 nlp_overrides:401] Model MegatronGPTModel was successfully restored from /workspace/nemo/demo/artifacts/final_model_checkpoints:latest/nemo_assets/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-08-16 09:36:59 auto_tokenizer:172] 15 special tokens added, resize your model accordingly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:36:59 save_restore_connector:249] Model MegatronGPTPromptLearningModel was successfully restored from /workspace/nemo/demo/artifacts/final_model_checkpoints:latest/NeMo_Megatron_PTuning.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-16 09:36:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/plugins/environments/torchelastic.py:36: UserWarning: MASTER_ADDR environment variable is not defined. Set as localhost\n",
      "      rank_zero_warn(\"MASTER_ADDR environment variable is not defined. Set as localhost\")\n",
      "    \n",
      "[NeMo W 2023-08-16 09:36:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/plugins/environments/torchelastic.py:44: UserWarning: MASTER_PORT environment variable is not defined. Set as 12910\n",
      "      rank_zero_warn(\"MASTER_PORT environment variable is not defined. Set as 12910\")\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    entity=\"a-sh0ts\",\n",
    "    project=\"NeMo_Megatron_PTuning\",\n",
    "    name=f\"export@{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n",
    "    config=args,\n",
    ")\n",
    "args = run.config\n",
    "squad_art_path = run.use_artifact(\"squad:latest\", type=\"datasets\").download()\n",
    "SQUAD_DIR = os.path.join(squad_art_path, \"data\", \"SQuAD\")\n",
    "\n",
    "# TODO: Loop all over artifact versions given a filter and use that to apply an alias for model registry\n",
    "final_chkpt_path = run.use_artifact(\n",
    "    \"final_model_checkpoints:latest\", type=\"model\"\n",
    ").download()\n",
    "tuned_model_path = os.path.join(final_chkpt_path, \"NeMo_Megatron_PTuning.nemo\")\n",
    "gpt_model_file = os.path.join(\n",
    "    final_chkpt_path, \"nemo_assets\", \"megatron_gpt_345m.nemo\"\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = args.output_dir\n",
    "NEMO_DIR = os.path.join(OUTPUT_DIR, \"nemo_assets\")\n",
    "CONFIG_DIR = os.path.join(NEMO_DIR, \"conf\")\n",
    "\n",
    "os.makedirs(NEMO_DIR, exist_ok=True)\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "\n",
    "# Download the example config file\n",
    "download_file(\n",
    "    f\"https://raw.githubusercontent.com/NVIDIA/NeMo/stable/examples/nlp/language_modeling/conf/megatron_gpt_prompt_learning_inference.yaml\",\n",
    "    CONFIG_DIR,\n",
    ")\n",
    "\n",
    "# Load the example config file so we can start editing it\n",
    "CONFIG_PATH = os.path.join(\n",
    "    CONFIG_DIR, \"megatron_gpt_prompt_learning_inference.yaml\"\n",
    ")\n",
    "cfg = OmegaConf.load(CONFIG_PATH)\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "\n",
    "# Override configuration values with command line arguments\n",
    "cfg.inference.greedy = args.greedy\n",
    "cfg.inference.top_k = args.top_k\n",
    "cfg.inference.top_p = args.top_p\n",
    "cfg.inference.temperature = args.temperature\n",
    "cfg.inference.add_BOS = args.add_BOS\n",
    "cfg.inference.tokens_to_generate = args.tokens_to_generate\n",
    "cfg.inference.all_probs = args.all_probs\n",
    "cfg.inference.repetition_penalty = args.repetition_penalty\n",
    "cfg.inference.min_tokens_to_generate = args.min_tokens_to_generate\n",
    "cfg.inference.compute_logprob = args.compute_logprob\n",
    "cfg.inference.batch_size = args.batch_size\n",
    "\n",
    "cfg.virtual_prompt_model_file = tuned_model_path\n",
    "cfg.model = {\n",
    "    \"language_model_path\": gpt_model_file,\n",
    "    \"virtual_prompt_style\": VirtualPromptStyle.P_TUNING.value,\n",
    "}\n",
    "cfg.gpt_model_file = gpt_model_file\n",
    "test_data_path = os.path.join(SQUAD_DIR, \"squad_short_test.jsonl\")\n",
    "cfg.data_paths = [test_data_path]\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# let's modify some trainer configs\n",
    "# check if we have GPU available and uses it\n",
    "accelerator = args.accelerator\n",
    "cfg.trainer.accelerator = accelerator\n",
    "cfg.trainer.devices = args.devices\n",
    "cfg.trainer.enable_checkpointing = args.enable_checkpointing\n",
    "\n",
    "# for PyTorch Native AMP set precision=16\n",
    "cfg.trainer.precision = args.precision\n",
    "\n",
    "# setup cluster environment parameters\"\n",
    "# use torch elastic cluster environment so `create_process_externally` is True\n",
    "# the launcher is set to None. It will not try to spawn new processes.\n",
    "# It won't create the misconfiguration error because of the `interactive session`\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise EnvironmentError(\"GPU is needed for the inference\")\n",
    "\n",
    "strategy = NLPDDPStrategy(\n",
    "    find_unused_parameters=False, no_ddp_communication_hook=True\n",
    ")\n",
    "plugins = [TorchElasticEnvironment()]\n",
    "trainer = Trainer(strategy=strategy, plugins=plugins, **cfg.trainer)\n",
    "\n",
    "# Set name of the experiment\n",
    "cfg.name = args.name\n",
    "cfg.exp_manager = {\n",
    "    \"resume_if_exists\": args.resume_if_exists,\n",
    "    \"create_wandb_logger\": args.create_wandb_logger,\n",
    "    \"wandb_logger_kwargs\": {\"project\": args.project, \"log_model\": args.log_model},\n",
    "}\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "\n",
    "# Init the experiment manager and view the exp_dir\n",
    "exp_dir = exp_manager(trainer, cfg.get(\"exp_manager\", None))\n",
    "exp_dir = str(exp_dir)\n",
    "\n",
    "if (\n",
    "    cfg.tensor_model_parallel_size < 0\n",
    "    or cfg.pipeline_model_parallel_size < 0\n",
    "    or cfg.get(\"pipeline_model_parallel_split_rank\", -1) < 0\n",
    "):\n",
    "    model_config = MegatronGPTPromptLearningModel.restore_from(\n",
    "        restore_path=cfg.gpt_model_file,\n",
    "        trainer=trainer,\n",
    "        return_config=True,\n",
    "    )\n",
    "\n",
    "    with open_dict(cfg):\n",
    "        cfg.tensor_model_parallel_size = model_config.get(\n",
    "            \"tensor_model_parallel_size\", 1\n",
    "        )\n",
    "        cfg.pipeline_model_parallel_size = model_config.get(\n",
    "            \"pipeline_model_parallel_size\", 1\n",
    "        )\n",
    "        cfg.pipeline_model_parallel_split_rank = model_config.get(\n",
    "            \"pipeline_model_parallel_split_rank\", 0\n",
    "        )\n",
    "\n",
    "assert (\n",
    "    cfg.trainer.devices * cfg.trainer.num_nodes\n",
    "    == cfg.tensor_model_parallel_size * cfg.pipeline_model_parallel_size\n",
    "), \"devices * num_nodes should equal tensor_model_parallel_size * pipeline_model_parallel_size\"\n",
    "\n",
    "# Update frozen GPT model path if it is given in case it has changed\n",
    "prompt_learning_cfg = MegatronGPTPromptLearningModel.restore_from(\n",
    "    cfg.virtual_prompt_model_file,\n",
    "    trainer=trainer,\n",
    "    return_config=True,\n",
    ")\n",
    "if cfg.get(\"gpt_model_file\"):\n",
    "    with open_dict(prompt_learning_cfg):\n",
    "        prompt_learning_cfg.language_model_path = cfg.gpt_model_file\n",
    "        prompt_learning_cfg.sequence_parallel = False\n",
    "        prompt_learning_cfg.activations_checkpoint_method = None\n",
    "        prompt_learning_cfg.activations_checkpoint_granularity = None\n",
    "        prompt_learning_cfg.activations_checkpoint_num_layers = None\n",
    "        prompt_learning_cfg.virtual_prompt_style = cfg.model.virtual_prompt_style\n",
    "\n",
    "# Load prompt tuned model, virtual_prompt_model_file must be provided in config\n",
    "# Now load prompt learning model with frozen gpt model base\n",
    "model = MegatronGPTPromptLearningModel.restore_from(\n",
    "    restore_path=cfg.virtual_prompt_model_file,\n",
    "    trainer=trainer,\n",
    "    override_config_path=prompt_learning_cfg,\n",
    ")\n",
    "model.freeze()\n",
    "\n",
    "# Have to turn off activations_checkpoint_method for inference\n",
    "try:\n",
    "    model.frozen_model.model.language_model.encoder.activations_checkpoint_method = (\n",
    "        None\n",
    "    )\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Check whether the DDP is initialized\n",
    "if parallel_state.is_unitialized():\n",
    "\n",
    "    def placeholder():\n",
    "        return\n",
    "\n",
    "    if model.trainer.strategy.launcher is not None:\n",
    "        model.trainer.strategy.launcher.launch(placeholder, trainer=model.trainer)\n",
    "    model.trainer.strategy.setup_environment()\n",
    "\n",
    "length_params: LengthParam = {\n",
    "    \"max_length\": cfg.inference.tokens_to_generate,\n",
    "    \"min_length\": cfg.inference.min_tokens_to_generate,\n",
    "}\n",
    "\n",
    "sampling_params: SamplingParam = {\n",
    "    \"use_greedy\": cfg.inference.greedy,\n",
    "    \"temperature\": cfg.inference.temperature,\n",
    "    \"top_k\": cfg.inference.top_k,\n",
    "    \"top_p\": cfg.inference.top_p,\n",
    "    \"repetition_penalty\": cfg.inference.repetition_penalty,\n",
    "    \"add_BOS\": cfg.inference.add_BOS,\n",
    "    \"all_probs\": cfg.inference.all_probs,\n",
    "    \"compute_logprob\": cfg.inference.compute_logprob,\n",
    "}\n",
    "\n",
    "config = OmegaConf.to_container(cfg.inference)\n",
    "model.set_inference_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadfefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe3a084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'taskname': 'squad',\n",
       "  'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       "  'question': 'Which NFL team represented the AFC at Super Bowl 50?'},\n",
       " {'taskname': 'squad',\n",
       "  'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       "  'question': 'Which NFL team represented the NFC at Super Bowl 50?'},\n",
       " {'taskname': 'squad',\n",
       "  'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       "  'question': 'Where did Super Bowl 50 take place?'},\n",
       " {'taskname': 'squad',\n",
       "  'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       "  'question': 'Which NFL team won Super Bowl 50?'},\n",
       " {'taskname': 'squad',\n",
       "  'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       "  'question': 'What color was used to emphasize the 50th anniversary of the Super Bowl?'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c5bcef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:37:24 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8af9dbc9ee4418bbfbbfb487f1248ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-16 09:37:28 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = (\n",
    "    model.frozen_model.cfg.encoder_seq_length - length_params[\"max_length\"]\n",
    ")\n",
    "max_seq_length = min(max_seq_length, cfg.get(\"max_seq_length\", 8192))\n",
    "\n",
    "_, dataloader = model.build_virtual_prompt_dataset(\n",
    "    data=cfg.data_paths,\n",
    "    batch_size=cfg.inference.get(\"batch_size\", 1),\n",
    "    max_seq_length=max_seq_length,\n",
    "    min_seq_length=model.cfg.data.get(\"min_seq_length\", 1),\n",
    "    add_bos=sampling_params[\"add_BOS\"],\n",
    "    add_eos=False,\n",
    "    for_train=False,\n",
    "    tokens_to_generate=length_params[\"max_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.get(\"num_workers\", 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558fbd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881e1060b5bf434583f4ee0c79a9cf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/modules/common/text_generation_utils.py:303: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/modules/common/text_generation_utils.py:311: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n",
      "[NeMo W 2023-08-16 09:39:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "response = trainer.predict(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "361db685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['taskname', 'context', 'question'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d25a3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_example = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7630344a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16485,   324],\n",
       "         [16485,   324],\n",
       "         [16485,   324],\n",
       "         [16485,   324],\n",
       "         [16485,   324]], device='cuda:0'),\n",
       " (tensor([[50256, 50257, 50258,  ..., 50256, 50256, 50256],\n",
       "          [50256, 50257, 50258,  ..., 50256, 50256, 50256],\n",
       "          [50256, 50257, 50258,  ..., 50256, 50256, 50256],\n",
       "          [50256, 50257, 50258,  ..., 50256, 50256, 50256],\n",
       "          [50256, 50257, 50258,  ..., 50256, 50256, 50256]], device='cuda:0'),\n",
       "  tensor([192, 192, 189, 189, 196], device='cuda:0')))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloader_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdacf764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentences', 'tokens', 'logprob', 'full_logprob', 'token_ids', 'offsets'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14703a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36edcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core.classes.exportable import Exportable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f0a0ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "from nemo.core.neural_types import ChannelType, NeuralType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "806ed7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportableSquadModel(torch.nn.Module, Exportable):\n",
    "    def __init__(self, model, trainer, input_example, cfg, max_seq_length, sampling_params, length_params):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.trainer = trainer\n",
    "        self._input_example = input_example\n",
    "        self.cfg = cfg\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.sampling_params = sampling_params\n",
    "        self.length_params = length_params\n",
    "        \n",
    "#     @property\n",
    "    def input_example(self):\n",
    "        return self._input_example\n",
    "        \n",
    "    @property\n",
    "    def input_module(self):\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def output_module(self):\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def input_names(self):\n",
    "        return ['taskname', 'context', 'question']\n",
    "    \n",
    "    @property\n",
    "    def output_names(self):\n",
    "        return ['sentences']\n",
    "    \n",
    "    @property\n",
    "    def input_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        return {\n",
    "            \"taskname\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"context\": NeuralType(('B', 'T'), ChannelType()),\n",
    "            \"question\": NeuralType(('B', 'T'), ChannelType()),\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n",
    "        return {\"sentences\": NeuralType(('B', 'T'), ChannelType())}\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        taskname,\n",
    "        context,\n",
    "        question\n",
    "    ):\n",
    "        model_input = [{\n",
    "            \"taskname\": taskname, \n",
    "            \"context\": context, \n",
    "            \"question\": question\n",
    "        }\n",
    "        ]\n",
    "        _, dataloader = self.model.build_virtual_prompt_dataset(\n",
    "            data=model_input,\n",
    "            batch_size=self.cfg.inference.get(\"batch_size\", 1),\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            min_seq_length=self.model.cfg.data.get(\"min_seq_length\", 1),\n",
    "            add_bos=self.sampling_params[\"add_BOS\"],\n",
    "            add_eos=False,\n",
    "            for_train=False,\n",
    "            tokens_to_generate=self.length_params[\"max_length\"],\n",
    "            drop_last=False,\n",
    "            shuffle=False,\n",
    "            num_workers=self.cfg.get(\"num_workers\", 1),\n",
    "        )\n",
    "        response = self.trainer.predict(self.model, dataloader)\n",
    "        output_response = []\n",
    "        for resp in response:\n",
    "            output_response.append({\n",
    "                \"sentences\": resp[\"sentences\"]\n",
    "            })\n",
    "        return output_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946d42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7bd9bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = ExportableSquadModel(model, trainer, data[0].values(), cfg, max_seq_length, sampling_params, length_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fbbabe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_model.forward(model_input=data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "974a6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.utils.export_utils import (\n",
    "    parse_input_example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "873c5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = export_model.input_module.input_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f83231ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['squad', 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'Which NFL team represented the AFC at Super Bowl 50?'])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7a864705",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list, input_dict = parse_input_example(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "96c928be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squad',\n",
       " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'Which NFL team represented the AFC at Super Bowl 50?']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "af87fda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "15f50c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-17 08:05:03 export_utils:430] Swapped 24 modules\n",
      "[NeMo I 2023-08-17 08:05:03 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403c6928a8e842cf86f3094c88eb98e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-17 08:05:03 gpt_prompt_learning_dataset:196] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46abeac0c80e4d67aea949130b1ef1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-17 08:07:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/gpt_prompt_learning_dataset.py:416: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:82.)\n",
      "      input_lengths = torch.cuda.LongTensor([len(inputs) for inputs in input_ids])\n",
      "    \n",
      "[NeMo W 2023-08-17 08:07:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2050: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input taskname\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 08:07:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2050: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input context\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 08:07:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:2050: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input question\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Diagnostic Run torch.onnx.export version 2.1.0a0+4136153 ===========\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MegatronBasePromptLearningModel.state_dict() got an unexpected keyword argument 'destination'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m export_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m export_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# or to('cpu') if you don't have GPU\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mexport_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexport_model.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nemo/core/classes/exportable.py:113\u001b[0m, in \u001b[0;36mExportable.export\u001b[0;34m(self, output, input_example, verbose, do_constant_folding, onnx_opset_version, check_trace, dynamic_axes, check_tolerance, export_modules_as_functions, keep_initializers_as_inputs)\u001b[0m\n\u001b[1;32m    111\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_export_subnet(subnet_name)\n\u001b[1;32m    112\u001b[0m out_name \u001b[38;5;241m=\u001b[39m augment_filename(output, subnet_name)\n\u001b[0;32m--> 113\u001b[0m out, descr, out_example \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_opset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_opset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Propagate input example (default scenario, may need to be overriden)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_example \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/nemo/core/classes/exportable.py:220\u001b[0m, in \u001b[0;36mExportable._export\u001b[0;34m(self, output, input_example, verbose, do_constant_folding, onnx_opset_version, check_trace, dynamic_axes, check_tolerance, export_modules_as_functions, keep_initializers_as_inputs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m get_dynamic_axes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_module\u001b[38;5;241m.\u001b[39minput_types_for_export, input_names)\n\u001b[1;32m    219\u001b[0m     dynamic_axes\u001b[38;5;241m.\u001b[39mupdate(get_dynamic_axes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_module\u001b[38;5;241m.\u001b[39moutput_types_for_export, output_names))\n\u001b[0;32m--> 220\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjitted_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_opset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_trace:\n\u001b[1;32m    235\u001b[0m     verify_runtime(\u001b[38;5;28mself\u001b[39m, output, check_trace_input, input_names, check_tolerance\u001b[38;5;241m=\u001b[39mcheck_tolerance)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:507\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    208\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1567\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1565\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1567\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1582\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1583\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1124\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1123\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1124\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1000\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    995\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    996\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     )\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1002\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:896\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_trace_and_get_graph_from_model\u001b[39m(model, args):\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# A basic sanity check: make sure the state_dict keys are the same\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# before and after running the model.  Fail fast!\u001b[39;00m\n\u001b[0;32m--> 896\u001b[0m     orig_state_dict_keys \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;66;03m# Disable Autocast cache because it replaces kernel's weight and bias\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;66;03m# by (undesired) constants.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# No perf impact for when there are reused weights since https://github.com/pytorch/pytorch/pull/85665\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;66;03m# TODO: https://github.com/pytorch/pytorch/issues/84092\u001b[39;00m\n\u001b[1;32m    902\u001b[0m     prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py:61\u001b[0m, in \u001b[0;36m_unique_state_dict\u001b[0;34m(module, keep_vars)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unique_state_dict\u001b[39m(module, keep_vars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# since Parameter.detach() always creates a new torch.Tensor instance,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# id(v) doesn't work with it. So we always get the Parameter or Buffer\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# as values, and deduplicate the params using Parameters and Buffers\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     filtered_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(state_dict)()\n\u001b[1;32m     63\u001b[0m     seen_ids: Set[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1817\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[0;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1819\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[0;31mTypeError\u001b[0m: MegatronBasePromptLearningModel.state_dict() got an unexpected keyword argument 'destination'"
     ]
    }
   ],
   "source": [
    "export_model.eval()\n",
    "export_model.to('cuda')  # or to('cpu') if you don't have GPU\n",
    "export_model.export(\"export_model.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c77a2ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexport_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "export_model.input_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57866649",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.log_code()\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
